---

layout: post
title: 神经网络模型优化
category: 技术
tags: MachineLearning
keywords: 深度学习

---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 前言

## 梯度下降法的实现方式

1. batch gradient descent
2. 随机梯度下降

## bias 偏差 和 variance 方差

[一文读懂深度学习：从神经元到BERT](https://www.jiqizhixin.com/articles/2019-05-28-5) 形象的说，拟和就是把平面上一系列的点，用一条光滑的曲线连接起来。因为这条曲线有无数种可能，从而有各种拟和方法。拟和的曲线一般可以用函数表示，根据这个函数的不同有不同的拟和的名字：欠拟合（underfitting） 和过拟合（overfitting）

![](/public/upload/machine/bias_variance.jpg)

假设存在多个数据集\\(D\_1,D\_2,...\\)，

1. `f(x;D)`由训练集 D 学得的模型 f 对 x 的预测输出。
2. y 表示x 的真实值
3. 针对所有训练集，学习算法 f 对测试样本 x 的 期望预测 为:

	$$
	\overline{f}(x)=E_D[f(x;D)]
	$$

4. 偏差，偏差度量了学习算法的期望预测与真实结果的偏离程度，即**刻画了学习算法本身的拟合能力**。

	$$
	偏差=(\overline{f}(x)-y)^2
	$$
5. 方差，方差度量了同样大小的训练集的变动所导致的学习性能的变化，即**刻画了数据扰动所造成的影响**。

	$$
	方差=E_D[(f(x;D)-\overline{f}(x))^2]
	$$

![](/public/upload/machine/bias_variance_model_complexity.png)


我们希望偏差与方差越小越好，但一般来说偏差与方差是有冲突的, 称为偏差-方差窘境 (bias-variance dilemma).

1. 给定一个学习任务, 在训练初期, 由于训练不足, 学习器的拟合能力不够强, 偏差比较大, 也是由于拟合能力不强, 数据集的扰动也无法使学习器产生显著变化, 也就是欠拟合的情况;
2. 随着训练程度的加深, 学习器的拟合能力逐渐增强, 训练数据的扰动也能够渐渐被学习器学到;
3. 充分训练后, 学习器的拟合能力已非常强, 训练数据的轻微扰动都会导致学习器发生显著变化, 当训练数据自身的、非全局的特性被学习器学到了, 则将发生过拟合.

![](/public/upload/machine/bias_variance_optimization.jpg)


1. 初始训练模型完成后，我们首先要知道算法的偏差高不高
2. 如果偏差很高，甚至无法拟合训练集，可以尝试

    1. **更大的网络** 比如更多的隐层或隐藏单元
    2. 花费更多的时间训练算法
    3. 更先进的优化算法
    4. 新的神经网络结构
    5. 准备更多的训练数据对高偏差没啥大用
3. 反复尝试，直到可以拟合训练集，使得偏差降低到可以接受的值
4. 如果方差比较高

    1. **更多的训练数据**
    2. regularization/正则化
    3. 新的神经网络结构
5. 不断尝试，直到找到一个低偏差、低方差的框架

## 防止过拟合

数学原理上理解起来还比较困难

### 正则化/规则化——给损失函数“加码”

[神经网络正则化(1)：L1/L2正则化](https://zhuanlan.zhihu.com/p/35893078)

假设一个神经网络样本为

$$\begin{Bmatrix}
(x^{(i)},y^{(i)}),i=1,...m
\end{Bmatrix}$$

训练过程中训练样本的预测结果为 

$$
\hat y^i,i =1,...m
$$
损失函数

$$
J(w,b)=\frac{1}{m}\sum\_{i=1}^mL(\hat y^i , y^i)
$$

L2正则化是给cost function加上正则项

$$
J(w,b)=\frac{1}{m}\sum\_{i=1}^mL(\hat y^i , y^i)+\frac{\lambda}{2m}||w||_2^2
$$

梯度下降时
$$
W=W-\sigma d_w
$$ 

加入L2 正则项之后，相当于在原来 \\(d_w\\) 的基础上带上 

$$
\frac{d}{d_W}(\frac{\lambda}{2m}W^2)=\frac{\lambda}{m}W
$$

\\(d_w\\) 变得更大，W会变得更小

L1正则化采用的正则化项如下所示

$$
J(w,b)=\frac{1}{m}\sum\_{i=1}^mL(\hat y^i , y^i)+\frac{\lambda}{2m}||w||_1
$$

为什么正则化有利于预防过拟合呢？为什么它可以减少方差问题？我们通过两个例子来直观体会一下

![](/public/upload/machine/bias_variance.jpg)

![](/public/upload/machine/neural_network_sample_example.jpg)

可以想象这是一个过拟合的神经网络。我们添加正则项，它可以避免数据权值矩阵过大，这就是弗罗贝尼乌斯范数，为什么压缩范数，或者弗罗贝尼乌斯范数或者参数可以减少过拟合？

直观上理解就是如果正则化设置得足够大，权重矩阵被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。但是会存在一个中间值，于是会有一个接近“Just Right”的中间状态。

直观理解就是增加到足够大，会接近于0，实际上是不会发生这种情况的，我们尝试消除或至少减少许多隐藏单元的影响，最终这个网络会变得更简单，这个神经网络越来越接近逻辑回归，我们直觉上认为大量隐藏单元被完全消除了，其实不然，实际上是该神经网络的所有隐藏单元依然存在，但是它们的影响变得更小了。

我们进入到神经网络内部来直观感受下为什么正则化会预防过拟合的问题，假设我们采用了tanh的双曲线激活函数

![](/public/upload/machine/tanhx.jpg)

如果使用了正则化部分，那么权重W会倾向于更小，因此得到的 \\(Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}\\) 会更小，在作用在激活函数的时候会接近于上图中横轴零点左右的部分。如果 Z 的值最终在这个范围内，都是相对较小的值， \\(g(z)=tanh(z)\\) 大致呈线性（上图横轴零点左右的部分大致是直线），**“边界”越线性，“弯曲度”就会小一点**，网络从过拟合逐步向高偏差状态靠拢。


### dropout 正则化

## 特征工程

[机器学习之 特征工程](https://juejin.im/post/5b569edff265da0f7b2f6c65) 是一个系列

特征： 是指数据中抽取出来的对结果预测有用的信息，也就是数据的相关属性。

特征工程：使用专业背景知识和技巧处理数据，使得 特征能在机器学习算法上发挥更好的作用的过程

![](/public/upload/machine/feature_enginering.png)

数据经过整理变成信息，信息能解决某个问题就是知识，知识通过反复实践形成才能，才能融会贯通就是智慧。 


